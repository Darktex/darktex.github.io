---
layout: post
category : blog
tags : [Machine Learning, Neural Nets, RNN, LSTM, Torch, JavaScript]
title: "Deploying a model trained with GPU in Torch into JavaScript, for everyone to use"
comments: on
languages: en
---

It's a great time to be into machine learning and artificial intelligence. Since deep learning took the world by storm in the last few years, there are new and exciting papers coming out almost every day. The speed at which the state of the art evolves is startling and indeed it is very hard to keep up! I've had this problem myself and I decided to spend some time on a fun project to better understand these sequence models everyone is so hyped about. [LSTMs (Long-short term memories)](https://en.wikipedia.org/wiki/Long_short-term_memory) have been around for quite some time (they were introduced in 1998, which is AGES ago for this field!), but it's only recently that the community has managed to train them on huge data and at scale (they had a variety of problems about which you can read [here](http://cs224d.stanford.edu/notebooks/vanishing_grad_example.html), for example). An excellent explanation can be found in [Chris Olah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), which is also the source of the following picture.

![LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)

As the field's boom is still quite recent, there has been only little effort in enabling users to take advantage of these techniques without specific equipment and/or lengthy installations of frameworks. It shouldn't be like this and I believe there hasn't been enough effort in abstracting the complexity away from the end user yet. That being said, there is some previous work in this direction: [Andrej Karpathy](http://cs.stanford.edu/people/karpathy/) shared a suite of JavaScript modules that can run in your browser. They are completely cross-platform (Windows, Mac, Linux, but also Android and iOS) and they can be used for a variety of tasks. He divided his code into three libraries, that cover three different applications: [ConvNetJS](http://cs.stanford.edu/people/karpathy/convnetjs/), [REINFORCEjs](http://cs.stanford.edu/people/karpathy/reinforcejs/) and [RecurrentJS](http://cs.stanford.edu/people/karpathy/recurrentjs/).

However, while these tools are great for some simple applications, they take a very long time to train a good model - we are talking days. The reason why they are so slow is because they use the CPU to do all the calculations. One of the biggest advances in the field and - in my opinion - the biggest enabler for its recent success has been the discovery that graphic cards can be used instead of a CPU for these task, and they work __a lot__ faster[^nvidia]. This is an amazing success in itself: a good GPU costs 300$ and that is already enough to train pretty good models! However, what was missing was the possibility to train a model on a GPU and deploy it so that everyone could use it, without buying or installing anything[^deployment].

The code that I'm releasing today does just that and acts as a bridge between two projects Karpathy made: char-rnn and RecurrentJS. I'm releasing all the code on [my GitHub](https://github.com/Darktex/char-rnn), as well as [a demo](http://testuggine.ninja/Trumpinator/) that I'm confident most will find amusing.

It runs quite fast (almost instantaneously on the few machines I have tried it on) despite running a decently sized model (the model has two stacked LSTMs, each with 300 units).

Some technical challenges that I had to face:

1. LUAJIT doesn't handle objects bigger than 1 GB. This caused a problem when unit testing my code on a bigger net, as I was incorporating the model into a big LUA table before writing the whole object into JSON. I solved it simply by creating the JSON on the fly, as I'm reading the binary with the model.
2. JSON files are much bigger than binaries. This is of course somehow inevitable as JSON is nowhere as efficient as a binary representation. That being said, there is still something that can be done. [The scientific literature](http://arxiv.org/pdf/1412.7024.pdf) shows that not only there is no loss of accuracy in storing a network's weights in single precision instead of double precision, but there is also a very small loss of accuracy in taking this into the extreme and using only 16 bits per weight ("half precision"). In fact, this approach has been so successful that nVidia now supports the type in its CUDA libraries. However, [Torch does not support it yet](https://github.com/soumith/cudnn.torch/issues/79) so I had to be a bit creative. In the end, I just truncated the weights to 5 digits (close to what a FP16 number would look like), but I also left users the option to choose other numbers. The model in the demo runs with 5 digits per weight.
3. LUA's strings are a bit weird. I was never able to print single and double quotes on their own, even though I needed to. In the end, I resorted to a hack to unblock me which was to store those characters as "SINGLEQUOTE" and "DOUBLEQUOTE", respectively. I modified ConvNetJS's code to convert them back to the right character when they read the model. This unfortunately means that I could not make my code completely transparent to ConvNetJS, but I figured that this can be easily fixed in a future release (and maybe the community can help with this).

All in all, this was a fun project. Getting funny quotes does take a while, but in the end I got my robot Trump to say that Italians "want to take our jobs". That did genuinely make me laugh, and it was a nice reward in itself.



[^nvidia]: nVidia [claims](http://www.nvidia.com/object/machine-learning.html) GPUs are usually between 10 and 100 times faster than a CPU.
[^deployment]: Running a pre-trained model takes a fraction of the time required to train it, so CPUs can do just fine on this task.
