---
layout: post
category : notes
title: Feasibility of learning
tags : [machine learning, caltech course, mooc]
comments: on
languages: en
---

These notes are written in English and refer to the fantastic Caltech online course you can find here: <http://work.caltech.edu/telecourse.html>. It is also worth noting you can find it on iTunes U, and if you have an iPad this makes a fantastic combo :-) The book is here: <http://www.amazon.com/gp/product/1600490069>. I make ample use of Yaser's material, including linking the relevant lecture PDF in each note and using his pictures (this is much faster than drawing stuff myself). Please note this is just for personal use and I give ALL the due credit to him!

A disclaimer on my notes in general: they are meant for my personal use and, although I'd love to know they helped somebody else, they are not written with clarity for third parties in mind. However, if you do use them and you would like to point out any errors, please write me an email!

##Refining the theory: feasibility

RELEVANT SLIDES: [Lecture 2](/assets/CaltechML/lecture2.pdf)

What the previous diagram lacks is an explanation about the __feasibility__ of the learning process. What we would like to have is a mathematical explanation to tell us _how well_ we can learn with the data we have.

###Hoeffding's inequality

To do that, we need a mathematical tool, __Hoeffding's inequality__. I failed to prove this formula, but the derivation carries nonetheless some interesting insights. It would be long to post here, so just look at [the note about the theory of Hoeffding's inequality](/notes/hoeffding-inequality/).

Here, we are just content with looking at the formula:


$$ P(\textbar \mu - \nu \textbar > \epsilon) \leq 2 e^{-2 \epsilon^2 m} $$

![bins](/images/notes/bins1.png)

__Notation:__ The notation for the formula is different here than in the note about Hoeffding's inequality. Here, we have $$ m $$ instead of $$ n $$ to be consistent with the notation I used before. You have $$ m $$ training examples, and this is the same $$ m $$ you want to plug in the formula.

This inequality is important because it puts an __upper bound__ on a __bad__ event (that the in-sample mean is too far away from the out-of-sample mean that we wanted to learn, i.e. our model sucks).

Now the question becomes: "How do I use this in learning?".

###Applying Hoeffding to the learning process

The learning process involves using a learning algorithm to search the space $$ H $$ (the __hypothesis set__), trying various $$ h $$ functions until it finds what it thinks is the best function, and call it $$ g $$ and output it.

However, the performance of $$ g $$ does not depend on this process alone: it depends on the __data__ as well (if not more!). For example, if you only have 1 training example, or if you have lots of useless data (for example, full of points that are all close to each other, but then you want to predict a point which is very far away), then there is no hypothesis set and no algorithm that will help you!

To help us formalize this, we modify our learning diagram a little bit and introduce a __probability distribution__ that generates the training examples. 

![new learning diagram](/images/notes/learningdiagram2.png)

Note that the distribution can be __anything__ and therefore it does not restrict our domain in any sense. The importance of it is that we now have a way to quantify the qualitative difference in the example, and this is where __Hoeffding's inequality__ kicks in: instead of thinking about balls in a jar, we now think about how well a function can predict outcomes given a training set generated by some probability distribution.

This time, the bin becomes the domain $$ X $$ and each marble ball is therefore a point $$ \textbf{x} \in X $$. We still define $$ \mu $$ as the fraction of green marble balls out of sample (in the whole $$X$$ now) and $$ \nu $$ as the fraction of green marble balls in sample (of length $$ m $$), but this time we change the  __meaning__ of the balls color. Now a green ball means $$ h(\textbf{x}) = f(\textbf{x}) $$ and a red ball means $$ h(\textbf{x}) \neq f(\textbf{x}) $$, therefore we see $$ f $$ and $$ h $$ as responsible of the coloring of the balls, altough not directly: none of them colors a ball alone, you need both to determine the color of the ball. They do not appear directly in the process: they are somewhere, like Platonic ideas, and together they influence the color of the marbles, which in turn give us a value of $$ \mu $$ for the whole bin and a value of $$ \nu $$ in each sample we draw. Notice how __the meaning of the accordance between $$ \mu $$ and $$ \nu $$ is not accuracy of the model, but rather accuracy of the TEST__. In other words, we use a test set of limited size and find out its performance $$ \nu $$. Then, Hoeffding will help us tell how indicative $$ \nu $$ is of the performance out-of-sample $$ \mu $$. Without this formalization, there can be no __verification__, let alone learning!

Summing up:

all the marble balls in a bin -> $$ X $$

each marble -> $$ \textbf{x} \in X $$

green ball -> $$ h(\textbf{x}) = f(\textbf{x}) $$

red ball -> $$ h(\textbf{x}) \neq f(\textbf{x}) $$

However, __this is not learning__. Learning involves using an algorithm to search a space $$ H $$ and try different functions $$ h \in H $$. Here, we have already picked some specific function and are testing its performance on a sample, using maths to guarantee the accuracy of the test within some threshold we are willing to tolerate.

![bins](/images/notes/bins2.png)

###Moving on to learning

Since one bin is clearly not enough, we use more bins. Each bin contains exactly the same marble balls ($$X$$ is the same), but the __colors will be different__ because each bin will have a different function $$ h $$ associated with it ($$ f $$ of course remains the same). For now, we assume the bins are in limited number[^limited] M.


![bins](/images/notes/bins3.png)

At this point, we cannot apply Hoeffding again to the whole process, because it is not a Bernoulli trial anymore. __Each single bin__ is, but the totality of the bins isn't.

Instead of relying on Hoeffding to find some upper bound on the bad probability (the probability of learning being useless because trained of a bad set), we assume we are in the worst possible case.

Imagine we have a coin. 

- If you toss it 10 times, the probability of getting 10 heads is P(10 heads on 10 tosses, 1 coin) $$ = (\frac{1}{2})^{10} \approx 0.1\% $$

- If you instead toss 1000 coins 10 times each, what is the probability that __at least one__ coin will get 10 heads? It is P(10 heads on 10 tosses, 1000 coins) $$ = 1 - [1 - (\frac{1}{2})^{10}]^{1000} \approx 62.3\% $$

The same can be said to our learning problem: if I have a set $$ H $$ containing 1000 functions $$h$$ I can choose from, what is the probability that __at least one__ sucks?

![bins](/images/notes/bins4.png)

We follow the very same reasoning: we want to know the probability of at least one failing.

$$ {\mathbb P}[ \textbar E_{in}(g) - E_{out}(g) \textbar > \epsilon ] \leq \\ \hspace{30pt} {\mathbb P}[ \textbar E_{in}(h_1) - E_{out}(h_1) \textbar > \epsilon ]  \\  
\hspace{30pt} \operatorname{OR}  {\mathbb P}[ \textbar E_{in}(h_2) - E_{out}(h_2) \textbar > \epsilon ]  \\\hspace{30pt} \operatorname{OR}  ... {\mathbb P}[ \textbar E_{in}(h_M) - E_{out}(h_M) \textbar > \epsilon ] $$


This can be bounded by the __union bound__[^unionbound], which intuitevely says that the maximum probability of at least an event occurring in N is when all the events are independent, in which case you just sum up the probabilities:

$$ {\mathbb P}\biggl(\bigcup_{i} A_i\biggr) \le \sum_i {\mathbb P}(A_i) $$

Therefore:

$$ {\mathbb P}[ \textbar E_{in}(g) - E_{out}(g) \textbar > \epsilon ] \le \sum_{i = 1}^M {\mathbb P}[ \textbar E_{in}(h_i) - E_{out}(h_i) \textbar > \epsilon ] \le \sum_{i = 1}^M 2 e^{-2 \epsilon^2 m} \le 2 M e^{-2 \epsilon^2 m} $$

The conclusion may seem both awkward and obvious, but __the bigger the hypothesis set, the higher the probability of at least one function being very bad__. In the event that we have an infinite hypothesis set, of course this bound goes to infinity and tells us nothing new.


[^unionbound]: Formalization and proof: <http://en.wikipedia.org/wiki/Union_bound>

[^smoothness]: Long story short, smoothness requires that infinitesimal variations in the domain reflect in infinitesimal variations in the codomain. This can be expressed as a requirement of differentiability. For a more formal definition, look at Wikipedia: <http://en.wikipedia.org/wiki/Smoothness>.

[^def]: Later, this will be expanded to any general distribution.

[^limited]: Later, this will be expanded to a possibly infinite set.
